Now Playing : Cloud Native Overview


The term cloud-native refers to the concept of building and running applications to take advantage of the distributed computing offered by the cloud delivery model. Cloud-native apps are designed and built to exploit the scale, elasticity, resiliency, and flexibility the cloud provides. As defined by the Cloud Native Computing Foundation, also abbreviated as CNCF, cloud-native technologies empower organizations to build and run scalable applications in public, private, and hybrid clouds.

Features such as containers, microservices, service meshes, declarative application programming interface, also known as APIs, and immutable infrastructure best illustrate this approach. These features enable loosely coupled systems that are resilient, manageable, and observable. They allow engineers to make high-impact changes frequently with minimal effort.

The modern landscape of complex applications with user expecting continuous innovation coupled with unparalleled responsiveness requires business systems to be more strategic and increasingly flexible. Cloud-native is all about moving fast, while also remaining agile. Cloud-native services empowers modern application development using technologies such as Kubernetes, Docker, serverless functions, APIs, streaming processing with Kafka, and DevOps tool set.

Industry-leading cloud providers enable cloud tooling and services so that developers can reduce operational tasks and build applications faster. Cloud-native native gives developers a comprehensive, standards-based platform for building, deploying, and managing cloud-native applications that are designed as loosely-coupled microservices and packaged as lightweight containers. Which are centered around APIs for collaboration, managed through the agile DevOps process and highly-automated capabilities.

Let's try to understand why enterprises must embrace cloud-native development for their apps or services. The first point is enhanced customer experience. Now as clients have become more demanding, corporates try to raise the bar by introducing incremental improvements and constant updates to improve the clients' experiences. Businesses that fail to develop customer-centric models are bound to lose a competitive edge in the market.

Cloud-native applications facilitate frequent changes and iterations in real-time to give your business the competitive advantage required to make the customer experience better. With cloud-native software development backed by CI/CD pipelines, businesses can robustly develop and deliver solutions as per the clients' needs to ensure long-term success. Cloud-native software is written, tested, and deployed on the cloud that leads to robust release with reduced latency, downtime, and technical problems.

These applications also facilitate operational efficiency that every business needs to deliver enhanced customer experiences. Talking about cost-effectiveness, cloud-native initiatives help businesses shift their valuable resources to cloud and leverage as a service solution to convert their capital expenditure into operational expenditure. One of the good things about building apps using containers and microservices is that those pieces can function like building blocks later for other projects.

Since developers are using a single platform to build all their apps, cloud-native apps makes it easier for the developers to access and reuse components created for earlier projects. Businesses use management and monitoring tools for cloud-native apps that reduces the cost associated with technology infrastructure. Moreover, develop risk-proof applications with cloud-native technology that offer many benefits like unrestrained scalability, seamless app management, secure infrastructure, and productive collaboration with impressive cost-effectiveness.

The third point talks about ease of management. Automation is a huge part of cloud-native, which forms the major distinction between traditional and cloud-native applications. Cloud-native provides a useful app development process, more labeled and adaptable app infrastructure. It also empowers the corporate teams to manage the business resources by enabling automatic provisioning handling continuous business needs with auto-scaling and minimizing failure risk with resiliency.

Cloud-native architecture patents depends on microservices that result in minimizing the chances of failures. The benefits of cloud-native begin with DevOps that helps the teams to release faster updates and assess problem better. Applications that are built with cloud-native approach encourage agility, efficiency, and flexibility that drive business growth.

Digital transformation with better security measures and analytics. Now adopting cloud-native infrastructure is more about surviving the changes in the market and climbing rather than just a competitive edge. The ability to respond faster to customer requirements is a changer with cloud-native solutions.

This leads to a reduced time to market, faster bug fixes and release of updates. Practices such as DevSecOps, multi-factor authentication, limiting access, and so on, can help your team build code with security in mind, rather than thinking of it as a post-deployment issue. Incorporating tools such as artificial intelligence and machine learning, along with better statistics on consumer behavior, can be obtained for personalized customer experiences and better alignment of the business goals.


![image](https://github.com/qriz1452/oci/assets/112246222/00633f78-0b70-45fb-ad95-6e1cf073b606)


--------------

Now Playing : Cloud-Native, Cloud-Enabled, and Cloud-Based Applications


Cloud-native, Cloud-enabled, and Cloud-based, they are the same thing, right? Not quite, actually. And it makes a big difference to development. And here's why. Cloud native systems are constructed in the Cloud from scratch, to harness the power of popular public Cloud environments-- for example, Oracle Cloud Infrastructure.

These systems provide developers new and advanced deployment tools that allow for a speedy evaluation of the enterprise's overall architecture. The Cloud-enabled application is an application that has been moved to Cloud, but was originally developed for use in a conventional data center. These applications were originally designed in a monolithic fashion and depend on local resources and hardware, later migrated to the Cloud.

The applications cannot take advantage of shared services or resource pools, and as a result, struggle to deliver the scalability and resiliency of other Cloud applications. Cloud-enabled can be an approach for legacy applications, or as the first step towards Cloud adoption. Cloud-based is the middle ground between Cloud-native and Cloud-enabled.

If you want to leverage some of the capabilities of the Cloud, such as higher availability and scalability, but don't want to completely redesign your application to use Cloud Services, this may be an approach to consider. For example, if you move your in-house web applications to Oracle Cloud Infrastructure, you now have a Cloud-based application. So what exactly is the difference between Cloud-native and Cloud-based? Why it is so important, let's find out.

So we will be comparing the characteristics of Cloud-native application with that of Cloud-based applications. The first characteristics that we'll be comparing is the origin. Cloud native applications are built in the Cloud and deployed in the Cloud, truly accessing the power of Cloud Infrastructure, whereas Cloud-based applications are generally made in-house using on-legacy infrastructure, and are tweaked to be made remotely available in the Cloud.

Talking about design, Cloud-native applications are designed to be hosted as multitenant instances, due to the microservices architecture, whereas Cloud-based applications are made on in-house servers, hence, they don't have any multitenant instances. Let's talk about ease of use. The applications are flexible and built to scale. And because of the microservices architecture, areas of an app can be upgraded without any disruption under Cloud-native application, whereas in the Cloud-based applications, it requires manual upgrades, causing disruption and shutdown to the application.

Talking about pricing, the Cloud-native applications are generally cheaper because you are charged based on the per use policy by the Cloud providers, whereas Cloud-based applications are generally costlier, as they require infrastructure upgrades to accommodate the changing requirements. They are more expensive because you have to own the whole stack and may need to purchase hardware, power, and cooling, before the application can be deployed.

Talking about implementation, Cloud-native applications are faster to deploy because there is no overhead of any hardware or software to be deployed, whereas in Cloud-based application, the implementation is a bit slower because of the hardware provisioning or the software setup required before deploying the application. Talking about maintenance, in Cloud-native application, interruptions are limited because of the microservices architecture that it follows, whereas in Cloud-based applications, interruptions can occur because of hardware migrations or specialized software configurations.


![image](https://github.com/qriz1452/oci/assets/112246222/33c0bd95-7e1f-42b9-9f49-bc36314dbc30)

![image](https://github.com/qriz1452/oci/assets/112246222/ac6fe04d-7ce0-46b4-a3db-a7ce9f7177b2)


---------------
Now Playing : Key Pillars of Cloud Native Development
CNCF is a subsidiary of the Linux Foundation established in 2015. CNCF is an open-source software foundation dedicated to making cloud-native computing universal and sustainable.

With the growing popularity of the CNCF as a unifying and organizing force in the cloud-native ecosystem and organizations increasingly embracing multi-cloud and hybrid-cloud models, developers should have the flexibility to build and deploy their applications anywhere they choose without the threat of cloud vendor lock-in, which enables them to build greater products even faster. The Cloud Native Computing Foundation hosts critical components to those software stacks, which includes Kubernetes, Fluentd, Envoy, Prometheus, CoreDNS, OpenTracing, containerd, and many others.

CNCF serves as the neutral home for collaboration. And brings together the industry's top developers and users and vendors, including the six largest public cloud providers and many of the leading private cloud companies. Now let's take a look at the key pillars of cloud-native development.

The first pillar is microservices. Microservices enables you to design your application as a collection of loosely coupled services. With the microservices architecture, a large application can be broken down into modules where each module has its own data and supports specific business goals. With the help of microservices architecture, each service can be updated simultaneously.

There are no restrictions on the language used by the developers, which helps in managing the individual modules. It is easier to find bugs or restrict specific modules that have bugs without the need to bring down the whole application. It is also easier to roll back to a previous version in case of failures.

Containers. Containers are packages of software that contains all the necessary elements to run in any environment. Basically, microservices are packaged in containers that connect and communicate with each other via application programming interfaces. The individual containers can all be deployed simultaneously and even in different languages. Containerization mitigates the risk of any conflicts between languages, libraries, or frameworks.

As containers are portable with the ability to operate in isolation from one another, it is effortless to create a microservices architecture using containers and move them to any environment as required. It is very much possible that your microservices might grow huge in numbers and then you need a way to manage those many containers. And in that situation, container orchestrator tools such as Kubernetes comes to the rescue.

Moving on. The next important pillar is DevOps. DevOps is a combination of development and operations brought together to create a unified infrastructure designed to maximize productivity. It creates a culture and an environment where building, testing, and releasing software happens rapidly.

CI/CD, also known as Continuous Integration and Continuous Delivery. So CI/CD pipelines helps you automate your software delivery process. This helps developers merge and test goods more frequently.

Continuous integration and continuous delivery helps you in making frequent releases and accelerate the go-to market process. At the same time, this allows you to move incremental software changes into production constantly through automation. Continuous delivery makes the act of releasing software robust and reliable so that the organizations can deliver software more frequently with less risk and improvise based on quick customer feedback.

Service mesh. A service mesh is a configurable low-latency infrastructure layer that controls the interaction between a network of microservices. As the number of microservices within an organization rise from single digit to numerous amounts, enter service complexities can become intimidating. Microservices pose challenges such as operational complexity, networking, communication between services, data consistency, and security. This is where service measures come in handy and are specifically designed to address these challenges by offering a granular level of control over how services communicate with each other.

By providing cloud-native capabilities and offerings regardless of the deployment scenario and leveraging open standards established by the Cloud-Native Computing Foundation, Oracle is a platinum member of CNCF and is uniquely providing its customers with choice, while meeting the broad deployment needs of developers. Oracle Cloud Native Services led to develop, deploy, and manage next generation applications that are containerized, serverless, infrastructure expressed as a code, and even driven. When combined with observability and messaging capabilities, they enable you to run highly scalable and available modern applications at a global enterprise level.


![image](https://github.com/qriz1452/oci/assets/112246222/6821f7e3-7b71-4837-ba2c-b79a62935c32)

![image](https://github.com/qriz1452/oci/assets/112246222/6cc90540-a171-497b-b96e-90afded106ad)

![image](https://github.com/qriz1452/oci/assets/112246222/7761b6f5-6431-4f94-aa62-524cc816d41b)


---------------

Now Playing : Benefits and Challenges of Cloud Native Development




Let's take a look at benefits of cloud-native development. Faster release. In today's world, the key to success for any organization is its speed of service. Cloud-native development allows companies to deliver features faster to the customer. Since they deliver faster, they usually gain a competitive advantage over their businesses. DevOps CI/CD pipelines plays a vital role in automating the development and the deployment processes.

Next is the ease of management. Cloud-native helps to make infrastructure management effortless. Using serverless platforms like OCI functions, they don't need to manage or worry about configuring networking, allocating storage, and provisioning cloud instances, et cetera.

Reduce cost. Cloud-native is very cost-effective through the pay-as-you-go system. Which means you will only pay for the resources you consume.

Reliable system. In Cloud applications, we use approaches like microservices and Kubernetes. It helps to build applications that fall tolerant with self-healing built in. When failures happen, the downtime and, in turn, the cost for legacy applications can be immense. With cloud-native apps, an incident can easily be isolated. And it does not impact the entire application.

Avoid vendor lock-in. Today, hybrid and multi-cloud are normal as it is a world of open--source and cloud technologies. Enterprises are not choosing just one infrastructure. They are making cloud-native applications that can run on any infrastructure, such as a public or private cloud, without any changes. So users can run an application on any platform without locking into one vendor's cloud.

Scalability. Cloud-native apps take advantage of the scalability that cloud offers. On-demand elasticity allows nearly limitless computing, storage, and other resources. Scalability is one of the hallmarks of the cloud-native approach and one of the primary drivers of its exploding popularity with businesses.

Auto-provisioning. Cloud-native application supports auto-provisioning. They will automatically go for self-service and programmatic provisioning when they need some resources. So this will help applications to run smoothly without the need for any manual intervention.

Making the transition from a traditional to cloud-native architecture comes with its own challenges. Let's take a look at some of the challenges of cloud-native development. Complex architecture. Cloud-native applications, which are microservices-based, are significantly more complicated than the legacy applications and need to be managed to scale efficiently.

Latest technology enhancements. As the time goes by, upgrading your system becomes increasingly challenging. If you don't keep pace with the technology enhancements, the market will overtake you before you even know it.

Continuous innovation. Cloud-native technologies needs constant innovation. If you fail to innovate, then the effort required to upgrade will grow exponentially over time. You cannot predict which tool will be most useful five years from now. However, it is important to stay relevant.

Next challenge is overdependence on a platform or provider. Through your cloud-native journey, if you have become excessively dependent on one technology or a provider, you might be stuck using tools or platforms even if they are not best fit for your needs as per the situation. It will also lead to added costs due to unproductive use of available technology and cloud capabilities.

Skills shortage. Talent acquisition is a major challenge in the technology sector. It's usually made worse when there is a skill gap due to evolving technologies and platforms. More so, when it comes to cloud-native development, it's difficult to hire and retain top talent.

Talking about security, microservices consists of many moving parts. And with many moving parts working simultaneously, security becomes a major concern. Nonetheless, it's important to maintain a practice around security that is deep-rooted into the team. Organizations can make use of DevSecOps to integrate security into the DevOps pipeline. Teams must build and code with security in mind and not leave it as an afterthought to be addressed when you do get hacked.

High operational and technology costs. Cloud-native infrastructures are complex entities that must be managed properly to scale cost-effectively. There are certain things you can do to improve your cloud usage. If not designed and utilized well, it can lead to budget overruns on cloud payments.


--------

Now Playing : Microservices Architecture: Overview


The term "microservices" is generally meant to describe an approach to software development that involves decomposing application functionality into individual components that can be deployed separately from each other and typically communicate via application programming interfaces or APIs.

Microservices basically utilize integration, API management, and cloud deployment technologies, which gives developers the freedom to independently develop and deploy services as per their requirement.

Microservices are used to design an application that is multilingual. This helps teams which work somewhat independently of each other. This allows developers to use different programming languages and technologies without affecting the overall architectural structure of the software development. One developer might use Node.js to code specific app feature, and the other might use Python. That's absolutely fine.

Loosely coupled with other services-- in a loosely coupled environment, you can easily work with the least elastic component to address any slowdowns or performance gaps. Loose couplings enables the isolation of microservices that lead to better productivity.

Easy to maintain and independently deployable-- following code is easier since the services are isolated and less dependent, which also makes it easier to deploy them in little pieces without affecting other services. So when a change is required in certain part of the application, only the related services can be modified and redeployed. There is no need to modify and redeploy the entire application.

Easily scalable and highly available-- microservices are easy to scale and integrate without third-party services. Although not a simple task, but maintaining high availability with microservices can be done using application methods like load balancing techniques and using API gateways.

Failure resistant-- you can build resilient microservices by including fault tolerance policies in your code. For example, in a movie ticket application, different microservices might support scheduling, purchasing, and customer preferences. If one service fails, fault tolerance policies help limit the error and keep it from taking down the whole application.

Let's take a look at a microservices architecture for a sample e-commerce application. The API layer is the entry point for all the client requests to a microservice. The API layer also enables microservices to communicate with each other using the HTTP, gRPC, and other TCP/UDP protocols.

The logic layer focuses on a single business task, minimizing the dependencies on the other microservice. This layer can be written in different language for each microservice. As you can see, this e-commerce application makes use of three microservices. One is coded in Java, the other one in Python, and the third one in Go. So this makes it multilingual.

The data store layer provides a persistence mechanism, such as a database storage engine, log files, and so on. Developers can consider using separate persistent data store for each microservice. As you can see over here, there is a separate data store for Container 2, and there's a separate data store for Container 3 that is using Go.

Typically, each microservice runs in a container that provides a lightweight runtime environment. And these can be scaled up or down as per the user requirements and managed using the orchestration tools like Kubernetes.

Let's understand the difference between a microservices architecture and a monolithic architecture. In a monolithic architecture, the entire application is built as a single unit, which contains the business logic, user interface, and data access layer. These three services are defined within the application as a single instance.

So basically, these three services will share the same resources and database, which makes them dependent on each other. The problem here is if one service within a monolithic application stops working, and then that application will stop.

On the other side, in the microservice architecture, the business logic is organized as multiple, loosely coupled services, where each service has their own resources, which make them independent autonomous services. So even if a single service crashes, it will not affect the other services running within the application.

Let's take a detailed look at how microservices differ from monolithic architectures. So when it comes to unit design, microservices architecture are loosely coupled services. Whereas monolithic architectures are designed, developed, and deployed as a single instance.

When it comes to functionality reuse, microservices define APIs that expose their functionality to any other client. Whereas in case of monolithic architecture, the functionality reuse is very limited. When it comes to communication within the application, microservices makes use of REST API calls based on the HTTP protocol. Whereas monolithic architectures make use of internal procedures and function calls.

Talking about technological flexibility, microservices architecture offer better flexibility as they show support for multiple programming languages and framework that best suits the problem. Whereas in monolithic architecture, you have to write the entire application using a single programming language.

Talking about data management, within microservices architecture, every microservice can manage their own data store, hence making it decentralized. Whereas in monolithic architecture, you must maintain a centralized database.

When it comes to deployment, microservices architecture can be deployed independently. Every microservice can be coded and deployed independently of each other. Whereas in monolithic architecture, you will have to write the entire code at once and then deploy it.

And again, if you have to make some changes, then you have to redeploy the entire application. Which is also why maintainability is easier in microservices architecture as you are managing small piece of codes at a time. Whereas in monolithic architecture it's very complex to maintain the entire piece of code.

Talking about resiliency and fault tolerance, microservices architectures show high resilience. Whereas monolithic architecture show low resilience. And at the end, talking about scalability, you can easily scale each microservice independently of the other services. Whereas in monolithic architecture, you must scale the entire application.

Communication mechanism in microservices architecture-- as we all know, microservices are distributed in nature. So to communicate with one another, they make use of inter-service communication on network level. Each microservice has its own instance and processes. Therefore, services must interact using an inter-service communication protocol like HTTP, gRPC, or message brokers like AMQP protocol.

Before we design our microservices communication, we should understand about communication styles. In general, there are two criteria to classify these communication systems-- based on the type of protocol that is either synchronous or asynchronous, and based on the number of receivers, which can be single or multiple.

Synchronous communication is using HTTP, HTTPS, or gRPC protocol for returning synchronized responses. The client usually sends a request and waits for a response from the service, which means client code logs that thread until they receive response from the service.

Talking about asynchronous communication-- in this case, the subprocesses are not logged, and protocols that are compatible with many operating systems and cloud environments are used. The most popular protocol for this asynchronous communication is AMQP protocol, that is advanced message queuing protocol. So when using AMQP protocols, the client sends the message using message broker systems. The popular ones are Kafka and RabbitMQ.

Remember, inter-service communication can result in a lot of network traffic. For that reason, serialization, speed, and payload size becomes more important. So if you are communicating between services internally within our microservices cluster, we might also use binary format communication mechanism like gRPC. GRPC is one of the best way to communicate for internal microservice communication.

The second classification criteria of microservice communication protocol is the number of receivers distinguishing between one or many receivers. In case of a single receiver, each request must be processed by a receiver or a service. One example of this type of communication is a command pattern.

And a good example, in case of many receiver, is the event-driven microservice architecture, which is based on a messaging agent or even bus interface that broadcasts the data updates among different microservices using events.

Normally, microservices-based applications use systems that combine different communication styles. The most common type is a single-receiver communication via an asynchronous protocol, with HTTP and HTTPS being one of the most used.

![image](https://github.com/qriz1452/oci/assets/112246222/ceab673d-24c9-4f1b-8318-6a611255dbb1)

![image](https://github.com/qriz1452/oci/assets/112246222/f35645bc-d194-48d4-9a31-b9019c9ad542)

![image](https://github.com/qriz1452/oci/assets/112246222/31435d9c-81f4-4344-ade3-9eb8152bfae5)



------------

Now Playing : Design Methodology of Microservices


The 12-factor methodology is a set of 12 best practices to develop applications which are supposed to run as a service. 12-factor app principles got very popular as it aligns with the microservices principle. Let's look at each of them.

The first one is codebase. This principle states that an app should be tracked in a single code repository and must not shared repository with any other application. Usually in microservices, every services have their own codebase. And having an independent codebase helps you to easy the CI/CD process for your applications.

The next one is dependencies. All the application packages should be managed through package managers like SBT, Maven, or Gradle. For example, Maven requires us to describe a project dependencies in an XML file, typically known as Project Object Model or POM. Again, in a noncontainerized environment, you can go for configuration management tools like Chef, Ansible, et cetera to install the system-level dependencies. But for a containerized environment, you can always go for a Docker file.

Configuration-- configuration says that you must externalize the configurations from the application. All the configuration data should be stored separately from the code in the environment as variables and not in the code repository and should be read in by the code at runtime. Having a separate config file makes it easy to update the config values without touching the actual codebase, thus eliminating the need for redeploying applications when the config values are changed. You can use configuration management tools like Ansible or Chef to automate this process.

Backing services-- a 12-factor app can automatically swap the application from one provider to another without making any further modifications to the codebase. Let us say you would like to change the database server from on-premises MySQL to OCI Database Cloud Service. To do so, you should not make any code changes to your application. Only configuration change should be able to take care of it.

Build, release, and run-- strictly separate the build and run stages is what this factor promotes. You can use the CI/CD tools to automate the builds and deployment process. Docker images make it easy to separate the build, release, and run stages more efficiently. Images should be created from every commit and treated as a deployment artifact.

Processes-- by adopting the stateless nature of REST, your services can be horizontally scaled as per the needs with zero impact. If your system still requires to maintain the state, you can use resources like Redis, Memcached or DataStore to store the state instead of in memory. Never assume that any caged-in memory or on-disk will be available during the feature request or job.

Port binding-- application should be self-contained instead of deploying them into any of the external web servers. To make the port-binding factor more useful for microservices, you must allow access to the persistent data owned by a service only by the way of service APIs. This prevents implicit service contracts between microservices.

Concurrency-- this principle advocates to opt in for the horizontal scaling instead of vertical scaling. While vertical scaling requires adding additional hardware to the system, horizontal scaling works by adding additional instances of the application. In microservices architecture, by adopting the containerization, applications can be scaled horizontally as per the demands.

Disposability-- the system should not get impacted when new instances are added or existing ones are taken down. This is known as system disposability. Systems do crash due to various reasons. The system should ensure that the impact would be minimal, and the application should be stored in a valid state. Services deployed in Docker containers do this automatically as it's an inherent feature of containers that they can be stopped and started instantly.

Development and production parity-- you must always keep the development, staging, and production as similar as possible to avoid the risk of bugs showing up in the later stages of your software development lifecycle. Containers work well for this as they enable you to run the exact same execution environment all the way from local development through the production.

Logs-- logs become paramount in troubleshooting the production issues for understanding the user behavior. Log provides visibility into behavior of a running application. Logs should be streamed to a chosen location rather than dumping them into a log file. Observability and monitoring help you achieve this in a microservices-based architecture, which we will be also covering in our later modules.

Admin processes-- the idea here is to separate the administrative task from the rest of the app to prevent one of task from causing issues with your running applications. For example, doing data cleanup, running analytics for a presentation, or turning on and off features for A/B testing.

After understanding microservices and differences between microservices and monolithic architecture, let us also look at the top benefits of microservices. The primary benefit of microservice architecture is its loosely-coupled components. These components can easily be developed, replaced, and scaled individually.

Microservices are not organized around technical capabilities of a particular product but rather business capabilities as the end goal is user experience and customer satisfaction. As with microservices, the choice of programming language can be multilingual. Also, the development teams can work in parallel on different pieces of code. It improves the productivity and, at the same time, the speed of deployment.

Increased fault tolerance and fault isolation-- microservices offer fallback mechanisms so that in case one microservice fails, the other microservices around it won't get hampered and they can still make progress. Greater scalability and flexibility-- this is one of the important features that microservices offer-- its ability to scale horizontally. This means that any deployed service can be duplicated in order to avoid slow execution bottlenecks.

Simplified security monitoring-- the ability to isolate each service facilitates better security management and monitoring. It's always easier to isolate threats with the modular arrangement. Autonomous, cross-functional teams-- specific microservices can be assigned to specific development teams, which allows them to focus solely on one service or feature this means teams can work autonomously without worrying what's going on with the rest of the application.

Let's take a look at some of the drawbacks of microservices design. Microservices designs are more complex. Communication between services can be complex at times. An application can include dozens or even hundreds of different services, and they all need to communicate securely.

More expensive than monoliths-- for microservices architecture to work for your organization, you need sufficient hosting infrastructure with security and maintenance support. You also need skilled development teams who will understand and manage all of these services for you. Any microservice initiative requires organizations to make changes to their internal culture. Before they can start the migration process, there should already be a mature, agile, and DevOps culture in place.

Microservice's design presents harder debugging problems. Debugging becomes more challenging with microservices. With an application consisting of multiple microservices and with each microservice having its own set of logs, tracing the source of the problem can be difficult at times.

Global testing is difficult. While unit testing may be easier with microservices, integration testing is not. The components are distributed, and developers can't test an entire system from their individual machines.

![image](https://github.com/qriz1452/oci/assets/112246222/26082c67-c0ed-429d-a78c-b884cc9b093d)

![image](https://github.com/qriz1452/oci/assets/112246222/499b3c5d-a297-4c89-8c37-9ca9cd04d64b)

![image](https://github.com/qriz1452/oci/assets/112246222/97d2c490-9d14-4e80-8df6-c41b38ff6c4c)



------------

Now Playing : DevOps: Overview



 Head & Heart Holistic Healthcare is a mid-sized business that develops healthcare and wellness software applications.

During the COVID pandemic, their business skyrocketed and they are being challenged to scale their development and operations very rapidly. The company started with the waterfall model of development 10 years ago, and since last five years, they've been using the agile model. However, they are struggling to keep up with the customer demand and the competitive marketplace. Let's meet our key players and listen to some of the specific challenges that everyone is facing.

Meet Jamal from the development team and Tricia from the operations team. The relationship between developers and IT operations is often adversarial. Typical challenges for developers such as Jamal include being pressured for on-time delivery, grappling with issues resolution, implementing new application capabilities, ongoing need for code management, and long and slow software release cycles.

Typical challenges for operational leads such as Tricia include resource contention, analysis and diagnostic checks, fast issue resolution, constant demand for redesign, and keeping up with the customer requests to edit or tweak. When a client has complains about a software, the blame is internally thrown at each other. The developer team would point fingers at the ITOps team, and then who would redirect the same blame to the dev team.

For example, Jamal would say the code worked perfectly on his machine, where Tisha can argue that the code submitted by Jamal was improper and it broke in the production environment. So overall, what developer wants is frequent changes to be pushed to production environment, whereas the operations want prod environment to be stable, but what business needs is both, that is quick changes and stability at the same time.

Meet Ramona, the CEO of Head & Heart Holistic Healthcare. She has heard from both her development and operational leads, and she knows they are struggling and working in their own silos, which is negatively impacting the company. She then makes a call to an Oracle Architect, Mo, and shares some of her specific concerns. She tells Mo about how her team can't seem to coordinate their production activities and tasks.

She then goes on to explain about the customer losses and complaints, as well as the mounting opportunity cost and the strategic impacts on the business and the financial plans. She asks for help. Mo, the Oracle Architect, listens patiently and then offers a resounding yes in order to help her overcome these challenges. Ramona and Mo spend an hour on the phone, and Ramona outlines her issues like deployment delays, high operational cost, communication issues, problems with security, high failure rate, erratic code execution, and no plan to implement continuous integration.

Mo takes notes, but knows all along that he has a perfect solution and asks Ramona if she has heard about the Dev-Ops service, and then goes on to explain to her regarding the OCI DevOps services. Now that we know the challenges a software development team faces, who uses a traditional software development approach, let's try to understand what DevOps is and how it helps overcome these challenges. Let's begin with DevOps definition. DevOps is a way of working that allows for continuous delivery.

DevOps encourages collaboration between teams such as development and operations who are working in silos. DevOps enables automation, basically using automation tools that can leverage an increasingly programmable and dynamic infrastructure, which helps in increasing the pace of releases, improve your product faster, and build competitive advantage. DevOps instills a continuous culture where it represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile lean practices in the context of system-oriented approach.

For DevOps to be effective, all what is required is a paradigm shift. Enterprises must make this paradigm shift to digital transformation. So we have defined what DevOps is. Now let's discuss why it matters. DevOps brings development and operations team together to create a unified infrastructure. And this team collaboration maximizes productivity, simplifies the software production lifecycle, makes each phase programmable and dynamic, helps you improve release and deployment frequency.

Along with maximizing predictability, efficiency, and security, it aids in release management and helps you find bugs early in the software development lifecycle. Let's look at the DevOps lifecycle. It is defined as a rapid release, multi-phased software development lifecycle. And in addition to having a built-in strong feedback loop, this lifecycle is test-driven to ensure robust software and uses code that is continuously built, tested, released, and monitored.

Now let's have a look at the lifecycle phases. On the dev side, you have plan, code, build, test. And on the operation side, you have release, deploy, operate, and monitor. So we have defined the DevOps software development lifecycle and it's 8 phases. Let's take another view and look at these phases laterally. So the phases are plan, code, build, test, release, deploy, operate, and monitor. The agile development covers phrases like plan, code, and build, where it organizes work in short iteration called sprints to increase the number of releases.

Agile emphasizes on software development methodology for developing the software. When the software is developed and released, the agile team will not care what happens to it. Continuous integration on the other hand, requires developers to integrate their work with one another as early as possible in the life cycle. This helps exposing bugs and issues on a regular basis. Continuous delivery provides tools and best practices to deliver quickly. At the end of each CI build, software is delivered for QA testing and then for delivery to the production environment.

And as you can see, continuous delivery stretches out up till the deploy phase. But what about continuous operation and monitoring? This is where DevOps comes into picture. DevOps is to give end-to-end business solution for faster delivery of products to the market. DevOps is all about taking software which is ready for release and deploying it in a reliable and secure manner. So now you know what CI and CD are, but as we said earlier, DevOps is an entire paradigm shift, and it instills a continuous culture.

That means that DevOps is all about continuity at every stage of the lifecycle. It's a process for iterative software development and is an umbrella over several other continuous processes, including continuous development, continuous testing, continuous release and deployment, continuous monitoring, and continuous feedback and optimization. Adopting DevOps to support the entire software development lifecycle with an emphasis on agility and efficiency regardless of function provides several benefits.

Some of these benefits include a curated culture, where your teams are more connected and the workers are more efficient, where you have greater employee engagement and collaboration with increased personal and professional growth opportunities. The other benefit includes tighter technicality, which provides improved overall software quality with more simplicity and less complexity.

It provides quick problem resolution and greater stability and reliability with continuous delivery. The DevOps benefit also leads to better business yields with lower IT cost, quick delivery times, more predictability, increased flexibility, and heightened innovation.




-----------

Now Playing : OCI DevOps Service

 let's generally define the DevOps in the Cloud. DevOps automation is becoming Cloud centric. Most public and private Cloud computing providers support DevOps systematically on their platform, including continuous integration and continuous development tools. This tight integration lowers the cost associated with on-premises DevOps automation technology and provides centralized governance and control for a sound DevOps process.

DevOps in Cloud is a suite of managed Cloud-based tools that forms a pipeline for code and bug tracking, automated unit testing and deployment, and reviewing and monitoring. Now the question is, why use DevOps in Cloud? There are several reasons why customers would want to use DevOps in the Cloud. Some of those reasons include it saves time and energy. Building DevOps infrastructure requires a lot of work. A pre-built solution will save time and energy.

Next is, it collapses into a single silo for collaboration. DevOps minimizes the silos and gaps by eliminating the boundaries between the conventional developer and the operation rules. This ensures business continuity, accountability, and desired results across groups. DevOps also helps you automate the entire deployment process. It helps you lower cost for development, testing, deployment, and operations. It's much easier to track cost of development resources and make adjustments as needed.

Let's understand OCI DevOps as a service in more detail. The Oracle Cloud infrastructure DevOps service is an end-to-end continuous integration and continuous delivery platform for developers. This service makes it easy to build, test, and deploy software and applications on the Oracle Cloud. With the help of OCI DevOps service, you can create private code repositories to store and manage source codes. You can also meet our external repositories from GitHub, GitLab, and Bitbucket Cloud to OCI code repositories.

It helps you build and test your latest changes in a build pipeline with the service-managed build runner. You can set up a trigger to automatically run your build pipeline from a source code commit or pull request. Optionally, you can run a deployment pipeline on the successful build run for a complete CI/CD automation. It lets you orchestrate your software deployment across regions to OCI platforms such as Container Engine for Kubernetes, also known as OKE, and compute instances and functions.

It helps you avoid downtime during deployments through the blue-green and canary deployment strategies. It helps you automate the complexity of updating applications. Automation reduces the chance of human error that might introduce a security vulnerability. It enhances the security and reduce risk in delivery as DevOps enables faster software delivery. Security bugs can be resolved quickly by rolling out a fix. Let's take a look at some of the DevOps as a service benefits.

The first benefit is automation. The OCI DevOps service fully automates the software delivery lifecycle and thereby increases the speed of development and reliability of application delivery. It eliminates manual error-prone methods of application integration and deployment and increases the reliability of operations. Next is scalability. OCI DevOps service scales your builds with service-managed build runners. You can run concurrent builds and don't need to manage or operate the underlying build run or host.

Integration and interoperability. DevOps service works with your existing Git repositories and continuous integration systems for customers building new Cloud native applications or migrating existing applications to OCI. The DevOps service is integrated with OCI services, such as virtual machines, Oracle container engine for Kubernetes, Oracle functions, artifact repositories, vault secrets policies, and much more. Low risk of deployments and faster time to market. With OCI DevOps deployment pipelines, you can reduce change-driven errors introduced by manual deployments. Instead, perform rolling, canary, or blue-green deployments, and optional automated rollbacks.

All these capabilities lead to reduce risk complexity and eliminate downtime of your production applications. Last but not the least is the low cost. For deployments, customer pay only for the resources used by the targets of the software deployments. 


![image](https://github.com/qriz1452/oci/assets/112246222/7a9b7d6b-7ce3-400a-ac25-5bacd948af53)


-------

Now Playing : OCI DevOps CI/CD


Continuous integration and continuous delivery or deployment is DevOps best practice. With continuous integration, shown in the blue on the screen, incremental frequent code changes are built, tested, and revised as needed based on constant feedback. The term refers to the fact that these small changes and error fixes are continuously integrated into the code repository.

Small frequent changes are easier to manage because errors are small, seniorly, and less likely to conflict with other code merges. CI/CD tools trigger automatic, standardized build and test steps that ensure the code changes being merged into the code repository are error-free and work with the existing code. This shifts the focus from problem detection to problem prevention, which is also called shifting left. As it moves code failures to earlier in the process, early fails are quicker and cheaper to fix.

Once the main code branch passes unit integration acceptance and other tests, it is then released to production in either a manual continuous delivery process or an automated continuous deployment mode, which is shown in the green on the image. Continuous feedback shown in the gray region is also a key part of the DevOps process. Here's the overall OCI DevOps structure and workflow.

Let's take a closer look at the OCI DevOps service offering. OCI DevOps project is a logical grouping of DevOps resources needed to implement a CI/CD workflow. DevOps resources can be artifacts, build pipelines, deployment pipelines, external connections, triggers, and environments.

The developer associate certification focuses on the services like container registry, OKE, and functions. But having knowledge of the OCI DevOps service, like build and deployment pipeline, can help you automate the process for continually integrating software development changes. You can learn more about the OCI DevOps service from the official documentation or by exploring the learning path on how to become an OCI DevOps professional.

Let's take a look and understand the OCI DevOps workflow right from committing the code changes until its deployment on the live servers. The heart of the workflow is the build and deployment pipelines with continuous integration and continuous deployment. The build pipeline follows a user-defined flow to build a code, test the package, and deliver them as code artifacts in the OCI artifact registry or as ready-to-ship image in the container registry.

The deployment pipeline then uses the build image from the container registry and the Kubernetes manifest to deploy the most recent version of the application to a staging environment. After the final tests and approvals, the pipeline pushes the image to the desired production environment, which can be a group of compute host, a container engine, or a function completely based on the architectural design of the software application.

You might also want to move an existing application from on-premises or another cloud to OCI, and move software delivery and deployments to the OCI platform. The OCI DevOps service also has the flexibility to integrate with your existing CI/CD workflows.


-----

Now Playing : OCI Code Editor Overview

. As a cloud developer, you and your team are tasked with creating and managing multiple workflows, projects, and code deployments across a wide range of OCI services. And this job often involves switching between local and remote development environments in various tools which leads to confusion as you switch contexts, costing you precious time and work focus.

Well, to help you be more productive and make development on OCI easier, you're encouraged to leverage Code Editor instead. The Oracle Cloud Infrastructure Code Editor is an in-console tool designed on the Eclipse Theia framework for efficient source code editing which enables you to edit and deploy code for various OCI services directly from the OCI Console.

You can now update service workflows and scripts without having to switch between the console and your local development environments. This streamlining makes rapid prototyping cloud solutions or exploring new services and accomplishing quick coding tasks much easier.

The Code Editor reduces your code development efforts through intelligent code editing features, such as syntax highlighting, code suggestions, bracket matching, linting, and code navigation. And this includes most OCI SDKs to include Java, Python, and JavaScript, along with dozens of other files syntax formats, such as JSON, HTML, and YAML.

In addition, direct access to the Cloud Shell from Code Editor enables you to edit files in your Cloud Shell virtual machine and natively compile, run, and deploy the code. You also have access to many cloud-based developer tools and utilities that are pre-installed with Cloud Shell, such as the OCI CLI, the kubectl client for Kubernetes clusters, the Fn CLI for Oracle Functions, Maven, and gradle, as well as many other common resources.

Another important feature is integrated source control through Git to enable team collaboration across multiple projects. You can use typical Git patterns to version and promote code directly from the Code Editor to include cloning of any Git-based repository and providing the ability to track file changes, commit, and pull and push code through intuitive UI elements. As an example, you can easily view changes made to files by simply clicking the M icon that is displayed once a file has been modified.

The Code Editor's plugin model provides a native integrated and customizable experience for supported OCI services offering specific functionality and coding workflows for each service. For example, launch Code Editor directly from the Functions console to easily edit and deploy functions code and automatically save code changes. You can also create a new or edit existing function configuration parameters.

You can edit API specification files directly in Code Editor which also provides an open API view pane that allows you to observe edit changes side by side between the text file and the graphic UI. Then that API spec file could then be used directly with the API Gateway to automatically define a new API deployment configuration.

Using the Resource Manager plugin, you can easily edit Terraform files and apply stock configurations directly from the Code Editor. This plugin also allows you to locate and import a resource manager stack bundle and quickly make changes without having to use any external or local IDE.

Oracle Data Science service allows for the creating of jobs which enable you to define and run a repeatable task on a fully managed infrastructure. Well, you can edit data science job artifacts easily from within Code Editor via the plugin by locating the data science project in your OCI tenancy.

A Code Editor workspace is simply a directory or multiple directories that stores files for a project. Well, when you first start Code Editor, your home directory becomes the default workspace. But you can create others as well. In addition, it will automatically create a workspace for you when you open a new folder.

More importantly, you can configure custom settings per workspace, including debug and task options or UI settings, such as window pane arrangements or font and color schemes. And Code Editor will persist the state of that workspace if you leave the environment and return to it later.

Speaking of state persistence, if your workflow is ever interrupted, such as a temporary local network disconnect or you just need to switch to other tasks, you can easily pick up right where you left off. Code Editor automatically saves your progress as you go and persists your user state so that, when Code Editor is relaunched, it returns you to your recent workspace, opens the last edited file you are working on, along with any other window panes and terminal sessions that were running.

And finally, there are a range of customizations that you can apply to your development environment to make it your own. You can control a complete personalization of fonts, color schemes, screen layouts, keyboard shortcuts, and language localization, both overall and at the workspace scope.

For example, you can choose from multiple dark and light color themes, control the relative placement of the Code Editor in Cloud Shell when open simultaneously, or modify the default key findings to match that of your favorite desktop code editor.

=======































































